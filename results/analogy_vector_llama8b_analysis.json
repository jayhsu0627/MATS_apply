{
  "model_name": "llama8b",
  "model_path": "meta-llama/Llama-3.1-8B-Instruct",
  "vector_shape": [
    32,
    4096
  ],
  "n_layers": 32,
  "layer_norms": [
    0.11181640625,
    0.2197265625,
    0.3203125,
    0.5234375,
    0.875,
    1.1484375,
    1.34375,
    1.765625,
    2.046875,
    2.46875,
    2.9375,
    2.859375,
    3.0625,
    3.828125,
    4.4375,
    4.8125,
    5.15625,
    6.53125,
    7.25,
    7.4375,
    7.84375,
    8.75,
    9.3125,
    10.75,
    11.6875,
    12.625,
    14.125,
    15.0625,
    17.5,
    19.125,
    22.25,
    29.375
  ],
  "best_layer": 31,
  "max_norm": 29.375,
  "mean_norm": 7.4375,
  "std_norm": 7.1875,
  "min_norm": 0.11181640625,
  "min_layer": 0,
  "top_5_layers": [
    31,
    30,
    29,
    28,
    27
  ],
  "top_5_norms": [
    29.375,
    22.25,
    19.125,
    17.5,
    15.0625
  ]
}